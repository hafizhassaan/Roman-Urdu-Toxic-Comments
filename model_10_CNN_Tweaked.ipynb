{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import All_RUT_Models\n",
    "import RUT_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters for this model\n",
    "\n",
    "max_len = 150\n",
    "embed_size = 300\n",
    "pre_trained_flag = True\n",
    "embed_trainable = False\n",
    "emb_weights_init = 'glorot_normal'\n",
    "spdrpt = 0.3\n",
    "drpt = 0.15\n",
    "conv_weights_init = 'glorot_uniform'\n",
    "conv_act = 'elu'\n",
    "fc_weights_init = 'glorot_uniform'\n",
    "fc_act = 'elu'\n",
    "lr_rate = 0.001\n",
    "optimizer = 'adam'\n",
    "multi_gpu_flag = 0\n",
    "gpus = 2\n",
    "batch = 64\n",
    "nepochs = 30\n",
    "patience = 7\n",
    "decay = True\n",
    "decay_rate = 0.5\n",
    "decay_after = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddingfile = './General_Embeddings/glove.txt'\n",
    "#embeddingfile = './General_Embeddings/w2v_cbow.txt'\n",
    "#embeddingfile = './General_Embeddings/w2v_sg.txt'\n",
    "#embeddingfile = './General_Embeddings/ft_cbow.vec'\n",
    "embeddingfile = './General_Embeddings/ft_sg.vec'\n",
    "\n",
    "embedding_matrix = []\n",
    "max_features = 100000\n",
    "\n",
    "modelname = 'CNN_Tweaked_ft_sg'\n",
    "\n",
    "modelpath = './Models/' + modelname + '/'\n",
    "\n",
    "if not os.path.exists( modelpath ):\n",
    "    os.makedirs( modelpath )\n",
    "if not os.path.exists( './Results/' ):\n",
    "    os.makedirs( './Results/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72771, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheets = [ pd.read_excel( 'RomanUrduToxicity.xlsx', 'Sheet1' ), pd.read_excel( 'RomanUrduToxicity.xlsx', 'Sheet2' ) ]\n",
    "df = pd.concat( sheets )\n",
    "df.reset_index( drop=True, inplace=True )\n",
    "df.Comment = df.Comment.astype( 'str' )\n",
    "del sheets\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs( word, *arr ):\n",
    "    return word, np.asarray( arr, dtype='float32' )\n",
    "\n",
    "def get_vectors( tokenizer ):\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min( max_features, len( word_index ) + 1 )\n",
    "    embedding_matrix = np.zeros( ( num_words, embed_size ) )\n",
    "    for word, i in word_index.items(  ):\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get( word )\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    gc.collect()\n",
    "    return embedding_matrix\n",
    "\n",
    "if pre_trained_flag == True:\n",
    "    embeddings_index = dict( get_coefs( *o.rstrip().rsplit(' ') ) for o in open( embeddingfile, encoding='utf-8' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold( n_splits=5, random_state=0, shuffle=True )\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 --MaxValF1: 0.85482782 --CurValF1: 0.85482782 --Patience: 00 --improved f1: 0.85482782\n",
      "Epoch: 001 --MaxValF1: 0.86225523 --CurValF1: 0.86225523 --Patience: 00 --improved f1: 0.86225523\n",
      "Epoch: 002 --MaxValF1: 0.86620870 --CurValF1: 0.86620870 --Patience: 00 --improved f1: 0.86620870\n",
      "Epoch: 003 --MaxValF1: 0.86620870 --CurValF1: 0.86531427 --Patience: 00\n",
      "Epoch: 004 --MaxValF1: 0.86620870 --CurValF1: 0.86375661 --Patience: 01\n",
      "Epoch: 005 --MaxValF1: 0.86620870 --CurValF1: 0.86459378 --Patience: 02\n",
      "Epoch: 006 --MaxValF1: 0.86620870 --CurValF1: 0.86117493 --Patience: 03\n",
      "Epoch: 007 --MaxValF1: 0.86620870 --CurValF1: 0.86410684 --Patience: 04\n",
      "Epoch: 008 --MaxValF1: 0.86620870 --CurValF1: 0.85843568 --Patience: 05\n",
      "Epoch: 009 --MaxValF1: 0.86620870 --CurValF1: 0.86208043 --Patience: 06\n",
      "Training stopped due to the patience parameter. --Patience: 07\n",
      "Fold: 01 out of 05 completed.\n",
      "Epoch: 000 --MaxValF1: 0.85017645 --CurValF1: 0.85017645 --Patience: 00 --improved f1: 0.85017645\n",
      "Epoch: 001 --MaxValF1: 0.85920105 --CurValF1: 0.85920105 --Patience: 00 --improved f1: 0.85920105\n",
      "Epoch: 002 --MaxValF1: 0.85920105 --CurValF1: 0.85798046 --Patience: 00\n",
      "Epoch: 003 --MaxValF1: 0.86301370 --CurValF1: 0.86301370 --Patience: 01 --improved f1: 0.86301370\n",
      "Epoch: 004 --MaxValF1: 0.86651209 --CurValF1: 0.86651209 --Patience: 00 --improved f1: 0.86651209\n",
      "Epoch: 005 --MaxValF1: 0.86651209 --CurValF1: 0.86242164 --Patience: 00\n",
      "Epoch: 006 --MaxValF1: 0.86651209 --CurValF1: 0.85638298 --Patience: 01\n",
      "Epoch: 007 --MaxValF1: 0.86651209 --CurValF1: 0.85456971 --Patience: 02\n",
      "Epoch: 008 --MaxValF1: 0.86651209 --CurValF1: 0.85984465 --Patience: 03\n",
      "Epoch: 009 --MaxValF1: 0.86651209 --CurValF1: 0.86320109 --Patience: 04\n",
      "Epoch: 010 --MaxValF1: 0.86651209 --CurValF1: 0.86512262 --Patience: 05\n",
      "Epoch: 011 --MaxValF1: 0.86651209 --CurValF1: 0.86433475 --Patience: 06\n",
      "Training stopped due to the patience parameter. --Patience: 07\n",
      "Fold: 02 out of 05 completed.\n",
      "Epoch: 000 --MaxValF1: 0.85924933 --CurValF1: 0.85924933 --Patience: 00 --improved f1: 0.85924933\n",
      "Epoch: 001 --MaxValF1: 0.86219803 --CurValF1: 0.86219803 --Patience: 00 --improved f1: 0.86219803\n",
      "Epoch: 002 --MaxValF1: 0.86544240 --CurValF1: 0.86544240 --Patience: 00 --improved f1: 0.86544240\n",
      "Epoch: 003 --MaxValF1: 0.86850756 --CurValF1: 0.86850756 --Patience: 00 --improved f1: 0.86850756\n",
      "Epoch: 004 --MaxValF1: 0.86850756 --CurValF1: 0.86776029 --Patience: 00\n",
      "Epoch: 005 --MaxValF1: 0.86850756 --CurValF1: 0.86723260 --Patience: 01\n",
      "Epoch: 006 --MaxValF1: 0.86850756 --CurValF1: 0.86716116 --Patience: 02\n",
      "Epoch: 007 --MaxValF1: 0.86850756 --CurValF1: 0.86249149 --Patience: 03\n",
      "Epoch: 008 --MaxValF1: 0.86857905 --CurValF1: 0.86857905 --Patience: 04 --improved f1: 0.86857905\n",
      "Epoch: 009 --MaxValF1: 0.86857905 --CurValF1: 0.86732673 --Patience: 00\n",
      "Epoch: 010 --MaxValF1: 0.86857905 --CurValF1: 0.86071670 --Patience: 01\n",
      "Epoch: 011 --MaxValF1: 0.86857905 --CurValF1: 0.86004057 --Patience: 02\n",
      "Epoch: 012 --MaxValF1: 0.86857905 --CurValF1: 0.86584962 --Patience: 03\n",
      "Epoch: 013 --MaxValF1: 0.86857905 --CurValF1: 0.86205777 --Patience: 04\n",
      "Epoch: 014 --MaxValF1: 0.86857905 --CurValF1: 0.86657946 --Patience: 05\n",
      "Epoch: 015 --MaxValF1: 0.86857905 --CurValF1: 0.86745407 --Patience: 06\n",
      "Training stopped due to the patience parameter. --Patience: 07\n",
      "Fold: 03 out of 05 completed.\n",
      "Epoch: 000 --MaxValF1: 0.86664472 --CurValF1: 0.86664472 --Patience: 00 --improved f1: 0.86664472\n",
      "Epoch: 001 --MaxValF1: 0.86762254 --CurValF1: 0.86762254 --Patience: 00 --improved f1: 0.86762254\n",
      "Epoch: 002 --MaxValF1: 0.87196541 --CurValF1: 0.87196541 --Patience: 00 --improved f1: 0.87196541\n",
      "Epoch: 003 --MaxValF1: 0.87261785 --CurValF1: 0.87261785 --Patience: 00 --improved f1: 0.87261785\n",
      "Epoch: 004 --MaxValF1: 0.87261785 --CurValF1: 0.86930693 --Patience: 00\n",
      "Epoch: 005 --MaxValF1: 0.87261785 --CurValF1: 0.87043418 --Patience: 01\n",
      "Epoch: 006 --MaxValF1: 0.87414966 --CurValF1: 0.87414966 --Patience: 02 --improved f1: 0.87414966\n",
      "Epoch: 007 --MaxValF1: 0.87766859 --CurValF1: 0.87766859 --Patience: 00 --improved f1: 0.87766859\n",
      "Epoch: 008 --MaxValF1: 0.87766859 --CurValF1: 0.87383178 --Patience: 00\n",
      "Epoch: 009 --MaxValF1: 0.87766859 --CurValF1: 0.86977219 --Patience: 01\n",
      "Epoch: 010 --MaxValF1: 0.87766859 --CurValF1: 0.86903962 --Patience: 02\n",
      "Epoch: 011 --MaxValF1: 0.87766859 --CurValF1: 0.87323944 --Patience: 03\n",
      "Epoch: 012 --MaxValF1: 0.87766859 --CurValF1: 0.87414188 --Patience: 04\n",
      "Epoch: 013 --MaxValF1: 0.87766859 --CurValF1: 0.86867305 --Patience: 05\n",
      "Epoch: 014 --MaxValF1: 0.87766859 --CurValF1: 0.86626747 --Patience: 06\n",
      "Training stopped due to the patience parameter. --Patience: 07\n",
      "Fold: 04 out of 05 completed.\n",
      "Epoch: 000 --MaxValF1: 0.86502660 --CurValF1: 0.86502660 --Patience: 00 --improved f1: 0.86502660\n",
      "Epoch: 001 --MaxValF1: 0.87334218 --CurValF1: 0.87334218 --Patience: 00 --improved f1: 0.87334218\n",
      "Epoch: 002 --MaxValF1: 0.87446174 --CurValF1: 0.87446174 --Patience: 00 --improved f1: 0.87446174\n",
      "Epoch: 003 --MaxValF1: 0.87446174 --CurValF1: 0.86991063 --Patience: 00\n",
      "Epoch: 004 --MaxValF1: 0.87446174 --CurValF1: 0.87112011 --Patience: 01\n",
      "Epoch: 005 --MaxValF1: 0.87446174 --CurValF1: 0.86809917 --Patience: 02\n",
      "Epoch: 006 --MaxValF1: 0.87446174 --CurValF1: 0.87339201 --Patience: 03\n",
      "Epoch: 007 --MaxValF1: 0.87446174 --CurValF1: 0.87200548 --Patience: 04\n",
      "Epoch: 008 --MaxValF1: 0.87446174 --CurValF1: 0.87398374 --Patience: 05\n",
      "Epoch: 009 --MaxValF1: 0.87446174 --CurValF1: 0.87292443 --Patience: 06\n",
      "Training stopped due to the patience parameter. --Patience: 07\n",
      "Fold: 05 out of 05 completed.\n",
      "Total runtime: 0:09:25.75\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings( 'ignore' )\n",
    "start_time = time.time()\n",
    "\n",
    "valaccuracy, valprecision, valrecall, valf1, valcm = [], [], [], [], []\n",
    "testaccuracy, testprecision, testrecall, testf1, testcm = [], [], [], [], []\n",
    "com_text, com_label, com_predicted, com_prob = [], [], [], []\n",
    "com_indices = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in skf.split( df.Comment, df.Toxic ):\n",
    "    # clearing previous sessions\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # tokenization with keras tokenizer\n",
    "    tokenizer = text.Tokenizer( num_words=max_features )\n",
    "    tokenizer.fit_on_texts( df.loc[ train_index ][ 'Comment' ].values )\n",
    "\n",
    "    traincomments = tokenizer.texts_to_sequences( df.loc[ train_index ][ 'Comment' ].values )\n",
    "    testcomments = tokenizer.texts_to_sequences( df.loc[ test_index ][ 'Comment' ].values )\n",
    "    \n",
    "    # pad the tokenized sequences\n",
    "    xtrain = sequence.pad_sequences( traincomments, maxlen=max_len )\n",
    "    xtest = sequence.pad_sequences( testcomments, maxlen=max_len )\n",
    "    \n",
    "    ytrain = df.loc[ train_index ][ 'Toxic' ].values\n",
    "    ytest = df.loc[ test_index ][ 'Toxic' ].values\n",
    "    \n",
    "    # split train and val\n",
    "    xtrain, xval, ytrain, yval = train_test_split( xtrain, ytrain, test_size=0.15, random_state=0 )\n",
    "    \n",
    "    # check if pre-trained word embeddings flag is true\n",
    "    if pre_trained_flag == True:\n",
    "        embedding_matrix = get_vectors( tokenizer=tokenizer)\n",
    "    \n",
    "    # define a model\n",
    "    model = All_RUT_Models.CNN_Tweaked( tokenizer=tokenizer, max_len=max_len, embed_size=embed_size,\n",
    "                                       embedding_matrix=embedding_matrix, embed_trainable=embed_trainable,\n",
    "                                       spdrpt=spdrpt, drpt=drpt, emb_weights_init=emb_weights_init,\n",
    "                                       conv_weights_init=conv_weights_init, conv_act=conv_act,\n",
    "                                       fc_weights_init=fc_weights_init, fc_act=fc_act, optimizer=optimizer,\n",
    "                                       multi_gpu_flag=multi_gpu_flag, gpus=gpus )\n",
    "    \n",
    "    K.set_value( model.optimizer.lr, lr_rate )\n",
    "    \n",
    "    # train the model with callbacks for early stopping\n",
    "    f1metric = RUT_Utils.F1Metrics( modelpath + modelname + str( fold ) + '.h5', patience=patience,\n",
    "                                   decay=decay, decay_rate=decay_rate, decay_after=decay_after, softmax=False )\n",
    "    hist = model.fit( xtrain, ytrain, batch_size=batch, validation_data=( xval,yval ),\n",
    "                     epochs=nepochs, verbose=0, callbacks=[ f1metric ] )\n",
    "    \n",
    "    # load saved model\n",
    "    loaded_model = load_model( modelpath + modelname + str(fold) + '.h5' )\n",
    "    \n",
    "    # get predictions (probabilities) for validation and test sets respectively\n",
    "    valpredictions = loaded_model.predict( xval, verbose=0, batch_size=2048 )\n",
    "    testpredictions = loaded_model.predict( xtest, verbose=0, batch_size=2048 )\n",
    "    \n",
    "    # optimizer threshold on validation set\n",
    "    threshold = RUT_Utils.optimize_threshold( yval, valpredictions )\n",
    "    \n",
    "    # save accuracy, precision, recall, f1 and confusion matrices\n",
    "    vallabels = (valpredictions>=threshold).astype( 'int32' )\n",
    "    testlabels = (testpredictions>=threshold).astype( 'int32' )\n",
    "    \n",
    "    valaccuracy.append( accuracy_score( yval, vallabels ) )\n",
    "    valprecision.append( precision_score( yval, vallabels ) )\n",
    "    valrecall.append( recall_score( yval, vallabels ) )\n",
    "    valf1.append( f1_score( yval, vallabels ) )\n",
    "    valcm.append( confusion_matrix( yval, vallabels ) )    \n",
    "    \n",
    "    testaccuracy.append( accuracy_score( ytest, testlabels ) )\n",
    "    testprecision.append( precision_score( ytest, testlabels ) )\n",
    "    testrecall.append( recall_score( ytest, testlabels ) )\n",
    "    testf1.append( f1_score( ytest, testlabels ) )\n",
    "    testcm.append( confusion_matrix( ytest, testlabels ) )\n",
    "    \n",
    "    # save for future analysis and ensemble\n",
    "    com_indices.extend( test_index.tolist() )\n",
    "    com_text.extend( df.loc[ test_index ][ 'Comment' ] )\n",
    "    com_label.extend( df.loc[ test_index ][ 'Toxic' ].tolist() )\n",
    "    com_predicted.extend( testlabels[:,0].tolist() )\n",
    "    com_prob.extend( testpredictions[:,0].tolist() )\n",
    "    \n",
    "    print( 'Fold: {:02d} out of {:02d} completed.'.format( fold, skf.get_n_splits() ) )\n",
    "    \n",
    "    fold = fold + 1\n",
    "time_took = time.time() - start_time\n",
    "print(f\"Total runtime: {hms_string(time_took)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy\n",
      "['0.9532', '0.9539', '0.9549', '0.9587', '0.9566'] 0.9554334134890645 +- 0.0019865107928273224 \n",
      "\n",
      "Validation Precision\n",
      "['0.8699', '0.8838', '0.8924', '0.9171', '0.8919'] 0.8910229101983141 +- 0.015381245866475584 \n",
      "\n",
      "Validation Recall\n",
      "['0.8625', '0.8499', '0.8460', '0.8415', '0.8577'] 0.8515204890014878 +- 0.007667908504861516 \n",
      "\n",
      "Validation F1\n",
      "['0.8662', '0.8665', '0.8686', '0.8777', '0.8745'] 0.8706860346715013 +- 0.004580823159118673\n"
     ]
    }
   ],
   "source": [
    "print( 'Validation Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valaccuracy ], np.mean( valaccuracy ), '+-', np.std( valaccuracy ), '\\n' )\n",
    "\n",
    "print( 'Validation Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valprecision ], np.mean( valprecision ), '+-', np.std( valprecision ), '\\n' )\n",
    "\n",
    "print( 'Validation Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valrecall ], np.mean( valrecall ), '+-', np.std( valrecall ), '\\n' )\n",
    "\n",
    "print( 'Validation F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in valf1 ], np.mean( valf1 ), '+-', np.std( valf1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1324  211]\n",
      " [ 198 7000]] \n",
      "\n",
      "[[1308  231]\n",
      " [ 172 7022]] \n",
      "\n",
      "[[1302  237]\n",
      " [ 157 7037]] \n",
      "\n",
      "[[1295  244]\n",
      " [ 117 7077]] \n",
      "\n",
      "[[1320  219]\n",
      " [ 160 7034]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in valcm:\n",
    "    print( np.rot90(np.rot90(c)), '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy\n",
      "['0.9483', '0.9488', '0.9492', '0.9439', '0.9480'] 0.9476439705208058 +- 0.001904368607708735 \n",
      "\n",
      "Test Precision\n",
      "['0.8869', '0.8924', '0.8913', '0.9066', '0.8889'] 0.8932292909719652 +- 0.006973980620971416 \n",
      "\n",
      "Test Recall\n",
      "['0.8168', '0.8137', '0.8175', '0.7675', '0.8125'] 0.8056028610652046 +- 0.01915638953989145 \n",
      "\n",
      "Test F1\n",
      "['0.8504', '0.8513', '0.8528', '0.8313', '0.8490'] 0.8469462905143592 +- 0.007938130015467478\n"
     ]
    }
   ],
   "source": [
    "print( 'Test Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testaccuracy ], np.mean( testaccuracy ), '+-', np.std( testaccuracy ), '\\n' )\n",
    "\n",
    "print( 'Test Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testprecision ], np.mean( testprecision ), '+-', np.std( testprecision ), '\\n' )\n",
    "\n",
    "print( 'Test Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testrecall ], np.mean( testrecall ), '+-', np.std( testrecall ), '\\n' )\n",
    "\n",
    "print( 'Test F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in testf1 ], np.mean( testf1 ), '+-', np.std( testf1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2140   480]\n",
      " [  273 11662]] \n",
      "\n",
      "[[ 2132   488]\n",
      " [  257 11677]] \n",
      "\n",
      "[[ 2141   478]\n",
      " [  261 11674]] \n",
      "\n",
      "[[ 2010   609]\n",
      " [  207 11728]] \n",
      "\n",
      "[[ 2128   491]\n",
      " [  266 11669]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in testcm:\n",
    "    print( np.rot90(np.rot90(c)), '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open( 'Results/ResultsMain.csv', mode='a' )\n",
    "file.write( modelname )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testaccuracy ))[:7] + '+-' + str(np.std( testaccuracy ))[:6] )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testprecision ))[:7] + '+-' + str(np.std( testprecision ))[:6] )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testrecall ))[:7] + '+-' + str(np.std( testrecall ))[:6] )\n",
    "file.write( ',' )\n",
    "file.write( str(np.mean( testf1 ))[:7] + '+-' + str(np.std( testf1 ))[:6] )\n",
    "file.write( '\\n' )\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72771, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPredictions = pd.DataFrame(  )\n",
    "dfPredictions[ 'comment_indices' ] = com_indices\n",
    "#dfPredictions[ 'comment_text' ] = com_text #comment text\n",
    "dfPredictions[ 'comment_label' ] = com_label\n",
    "dfPredictions[ 'comment_predicted' ] = com_predicted\n",
    "dfPredictions[ 'comment_prob' ] = com_prob\n",
    "dfPredictions.to_csv( 'Results/' + modelname + '.csv', index=False )\n",
    "dfPredictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
