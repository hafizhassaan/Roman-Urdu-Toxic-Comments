{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = [ pd.read_excel( 'RomanUrduToxicity.xlsx', 'Sheet1' ), pd.read_excel( 'RomanUrduToxicity.xlsx', 'Sheet2' ) ]\n",
    "dforiginal = pd.concat( sheets )\n",
    "dforiginal.reset_index( drop=True, inplace=True )\n",
    "dforiginal.Comment = dforiginal.Comment.astype( 'str' )\n",
    "del sheets\n",
    "dforiginal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "dfs.append( pd.read_csv( './Results/NB.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/RF.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/LR.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/SVM.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_George.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_P.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_GRU.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BLSTM.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_Tweaked.csv' ) )\n",
    "\n",
    "dfs.append( pd.read_csv( './Results/CNN_George_glove.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_P_glove.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_GRU_glove.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BLSTM_glove.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_glove.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_Tweaked_glove.csv' ) )\n",
    "\n",
    "dfs.append( pd.read_csv( './Results/CNN_George_w2v_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_P_w2v_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_GRU_w2v_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BLSTM_w2v_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_w2v_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_Tweaked_w2v_cbow.csv' ) )\n",
    "\n",
    "dfs.append( pd.read_csv( './Results/CNN_George_w2v_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_P_w2v_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_GRU_w2v_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BLSTM_w2v_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_w2v_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_Tweaked_w2v_sg.csv' ) )\n",
    "\n",
    "dfs.append( pd.read_csv( './Results/CNN_George_ft_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_P_ft_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_GRU_ft_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BLSTM_ft_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_ft_cbow.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_Tweaked_ft_cbow.csv' ) )\n",
    "\n",
    "dfs.append( pd.read_csv( './Results/CNN_George_ft_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_P_ft_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_GRU_ft_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BLSTM_ft_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/BGRU_ft_sg.csv' ) )\n",
    "dfs.append( pd.read_csv( './Results/CNN_Tweaked_ft_sg.csv' ) )\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = [ 'NB', 'RF', 'LR', 'SVM', 'CNN_George', 'BGRU_P', 'CNN_GRU', 'BLSTM', 'BGRU', 'CNN_Tweaked',\n",
    "             'CNN_George_glove', 'BGRU_P_glove', 'CNN_GRU_glove', 'BLSTM_glove', 'BGRU_glove', 'CNN_Tweaked_glove',\n",
    "             'CNN_George_w2v_cbow', 'BGRU_P_w2v_cbow', 'CNN_GRU_w2v_cbow', 'BLSTM_w2v_cbow', 'BGRU_w2v_cbow', 'CNN_Tweaked_w2v_cbow',\n",
    "             'CNN_George_w2v_sg', 'BGRU_P_w2v_sg', 'CNN_GRU_w2v_sg', 'BLSTM_w2v_sg', 'BGRU_w2v_sg', 'CNN_Tweaked_w2v_sg',\n",
    "             'CNN_George_ft_cbow', 'BGRU_P_ft_cbow', 'CNN_GRU_ft_cbow', 'BLSTM_ft_cbow', 'BGRU_ft_cbow', 'CNN_Tweaked_ft_cbow',\n",
    "             'CNN_George_ft_sg', 'BGRU_P_ft_sg', 'CNN_GRU_ft_sg', 'BLSTM_ft_sg', 'BGRU_ft_sg', 'CNN_Tweaked_ft_sg']\n",
    "mlmodels = [ 'NB', 'RF', 'LR', 'SVM' ]\n",
    "deepmodels = [ m for m in modelnames if m not in mlmodels ]\n",
    "ml_bestdeep = [ 'NB', 'RF', 'LR', 'SVM', 'CNN_George_ft_sg', 'BGRU_P_ft_sg', 'CNN_GRU_ft_sg', 'BLSTM_ft_sg', 'BGRU_ft_sg', 'CNN_Tweaked_ft_sg' ]\n",
    "bestdeep = [ 'CNN_George_ft_sg', 'BGRU_P_ft_sg', 'CNN_GRU_ft_sg', 'BLSTM_ft_sg', 'BGRU_ft_sg', 'CNN_Tweaked_ft_sg' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maj_vote( dataframe, alabel, models ):\n",
    "    accuracy, precision, recall, f1, yy_true, yy_pred = [], [], [], [], [], []\n",
    "    \n",
    "    dataframe[ 'maj_vote' ] = dataframe[ models ].sum( axis=1 )\n",
    "    dataframe[ 'maj_vote_res' ] = dataframe[ 'maj_vote' ].apply( lambda x:  1 if x/len(models) >= 0.5 else 0  )\n",
    "    \n",
    "    skf = StratifiedKFold( n_splits=5, random_state=0, shuffle=True )\n",
    "    skf.get_n_splits( dforiginal.Comment, dataframe[ alabel ] )\n",
    "    \n",
    "    i = 1\n",
    "    for train_index, test_index in skf.split( dforiginal.Comment, dataframe[ alabel ] ):\n",
    "        ytest = dataframe.loc[ test_index ][ 'comment_label' ].values\n",
    "        testpredictions = dataframe.loc[ test_index ][ 'maj_vote_res' ].values\n",
    "        \n",
    "        accuracy.append( accuracy_score( ytest, testpredictions ) )\n",
    "        precision.append( precision_score( ytest, testpredictions ) )\n",
    "        recall.append( recall_score( ytest, testpredictions ) )\n",
    "        f1.append( f1_score( ytest, testpredictions ) )\n",
    "        yy_true.extend( ytest.tolist() )\n",
    "        yy_pred.extend( testpredictions.tolist() )\n",
    "\n",
    "        i = i + 1\n",
    "    \n",
    "    return accuracy, precision, recall, f1, yy_true, yy_pred\n",
    "\n",
    "def avg_prob( dataframe, alabel, models ):\n",
    "    accuracy, precision, recall, f1 = [], [], [], []\n",
    "    \n",
    "    dataframe[ 'avg_prob' ] = dataframe[ models ].mean( axis=1 )\n",
    "    dataframe[ 'avg_prob_res' ] = dataframe[ 'avg_prob' ].apply( lambda x:  1 if x >= 0.5 else 0  )\n",
    "    \n",
    "    skf = StratifiedKFold( n_splits=5, random_state=0, shuffle=True )\n",
    "    skf.get_n_splits( dforiginal.Comment, dataframe[ alabel ] )\n",
    "    \n",
    "    i = 1\n",
    "    for train_index, test_index in skf.split( dforiginal.Comment, dataframe[ alabel ] ):\n",
    "        ytest = dataframe.loc[ test_index ][ 'comment_label' ].values\n",
    "        testpredictions = dataframe.loc[ test_index ][ 'avg_prob_res' ].values\n",
    "        \n",
    "        accuracy.append( accuracy_score( ytest, testpredictions ) )\n",
    "        precision.append( precision_score( ytest, testpredictions ) )\n",
    "        recall.append( recall_score( ytest, testpredictions ) )\n",
    "        f1.append( f1_score( ytest, testpredictions ) )\n",
    "\n",
    "        i = i + 1\n",
    "    return accuracy, precision, recall, f1, dataframe[ 'avg_prob' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflabels = pd.DataFrame()\n",
    "dflabels[ 'comment_indices' ] = dfs[0][ 'comment_indices' ]\n",
    "#dflabels[ 'comment_text' ] = dfs[0][ 'comment_text' ]\n",
    "dflabels[ 'comment_label' ] = dfs[0][ 'comment_label' ]\n",
    "\n",
    "for i in range( len( dfs ) ):\n",
    "    dflabels[ modelnames[i] ] = dfs[i][ 'comment_predicted' ]\n",
    "dflabels = dflabels.sort_values(by='comment_indices').reset_index( drop=True )\n",
    "dflabels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfprobs = pd.DataFrame()\n",
    "dfprobs[ 'comment_indices' ] = dfs[0][ 'comment_indices' ]\n",
    "#dfprobs[ 'comment_text' ] = dfs[0][ 'comment_text' ]\n",
    "dfprobs[ 'comment_label' ] = dfs[0][ 'comment_label' ]\n",
    "\n",
    "for i in range( len( dfs ) ):\n",
    "    dfprobs[ modelnames[i] ] = dfs[i][ 'comment_prob' ]\n",
    "dfprobs = dfprobs.sort_values(by='comment_indices').reset_index( drop=True )\n",
    "dfprobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_results( filename, mode, name, accuracy, precision, recall, f1 ):\n",
    "    file = open( filename, mode=mode )\n",
    "    file.write( name )\n",
    "    file.write( ',' )\n",
    "    file.write( str(np.mean( accuracy ))[:7] + '+-' + str(np.std( accuracy ))[:6] )\n",
    "    file.write( ',' )\n",
    "    file.write( str(np.mean( precision ))[:7] + '+-' + str(np.std( precision ))[:6] )\n",
    "    file.write( ',' )\n",
    "    file.write( str(np.mean( recall ))[:7] + '+-' + str(np.std( recall ))[:6] )\n",
    "    file.write( ',' )\n",
    "    file.write( str(np.mean( f1 ))[:7] + '+-' + str(np.std( f1 ))[:6] )\n",
    "    file.write( '\\n' )\n",
    "    file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, yy_true, yy_pred = maj_vote( dflabels, 'comment_label', modelnames )\n",
    "ensemble_results( './Results/ensemble.csv', 'w', 'All (maj_vote)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, prr = avg_prob( dfprobs, 'comment_label', modelnames )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'All (avg_prob)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, yy_true, yy_pred = maj_vote( dflabels, 'comment_label', deepmodels )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'All Deep (maj_vote)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, prr = avg_prob( dfprobs, 'comment_label', deepmodels )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'All Deep (avg_prob)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, yy_true, yy_pred = maj_vote( dflabels, 'comment_label', mlmodels )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'All ML (maj_vote)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, prr = avg_prob( dfprobs, 'comment_label', mlmodels )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'All ML (avg_prob)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, yy_true, yy_pred = maj_vote( dflabels, 'comment_label', ml_bestdeep )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'ML + Best Deep (maj_vote)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = confusion_matrix( yy_true, yy_pred )\n",
    "print( np.rot90(np.rot90(c)), '\\n\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, prr = avg_prob( dfprobs, 'comment_label', ml_bestdeep )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'ML + Best Deep (avg_prob)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, yy_true, yy_pred = maj_vote( dflabels, 'comment_label', bestdeep )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'Best Deep (maj_vote)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1, prr = avg_prob( dfprobs, 'comment_label', bestdeep )\n",
    "ensemble_results( './Results/ensemble.csv', 'a', 'Best Deep (avg_prob)', accuracy, precision, recall, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ' Accuracy' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in accuracy ], np.mean( accuracy ), '+-', np.std( accuracy ), '\\n' )\n",
    "\n",
    "print( ' Precision' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in precision ], np.mean( precision ), '+-', np.std( precision ), '\\n' )\n",
    "\n",
    "print( ' Recall' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in recall ], np.mean( recall ), '+-', np.std( recall ), '\\n' )\n",
    "\n",
    "print( ' F1' )\n",
    "print( [ '{:0.4f}'.format( x ) for x in f1 ], np.mean( f1 ), '+-', np.std( f1 ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
